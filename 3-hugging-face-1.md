https://colab.research.google.com/drive/1uq2pRe38ZqmsGhqyshSOOEwESc4EzZZ2#scrollTo=32vDDYCz_JaE

# 토크나이저의 CLS과 SEP

CLS와 SEP 토큰에 대해 설명드리겠습니다:

1. CLS 토큰 (Classification Token):

   - 의미: "Classification"의 약자
   - 목적:
     • 시퀀스의 시작을 나타냅니다.
     • 전체 시퀀스의 표현을 학습하는 데 사용됩니다.
   - 위치: 일반적으로 시퀀스의 맨 앞에 위치
   - 사용: 문장 분류, 감성 분석 등의 작업에서 중요한 역할

2. SEP 토큰 (Separator Token):
   - 의미: "Separator"의 약자
   - 목적:
     • 서로 다른 문장이나 시퀀스를 구분합니다.
     • 두 개의 문장을 입력으로 받는 작업(예: 질문-답변 쌍)에서 사용됩니다.
   - 위치: 문장의 끝이나 두 문장 사이에 위치
   - 사용: 문장 쌍 분류, 질문 답변 등의 작업

예시:

```
[CLS] 나는 학교에 간다 [SEP] 그는 집에 있다 [SEP]
```

이 구조는 BERT와 같은 트랜스포머 기반 모델에서 자주 사용됩니다. CLS와 SEP 토큰은 모델이 입력의 구조를 이해하고, 다양한 NLP 작업을 수행하는 데 도움을 줍니다.

이러한 특수 토큰들은 모델이 입력의 시작과 끝, 그리고 서로 다른 부분을 인식하는 데 중요한 역할을 합니다.

# attention mask가 뭐지?

attention mask는 트랜스포머 기반 모델에서 중요한 개념입니다. 주요 기능과 목적은 다음과 같습니다:

1. 패딩 처리:

   - 시퀀스의 길이가 다를 때 짧은 시퀀스에 패딩을 추가합니다.
   - attention mask는 모델이 패딩 토큰을 무시하도록 합니다.

2. 유효한 입력 표시:

   - 1은 유효한 토큰, 0은 패딩 토큰을 나타냅니다.
   - 예: [1, 1, 1, 1, 0, 0]은 4개의 유효 토큰과 2개의 패딩을 의미합니다.

3. 어텐션 계산 제어:

   - 모델이 유효한 토큰에만 집중하도록 합니다.
   - 패딩 토큰에 대한 불필요한 계산을 방지합니다.

4. 효율성 향상:

   - 패딩 토큰을 무시함으로써 계산 효율성이 높아집니다.

5. 다양한 길이 처리:
   - 배치 처리 시 다양한 길이의 시퀀스를 효과적으로 다룰 수 있게 합니다.

# 패딩 토큰 (Padding Token):

패딩 토큰이 필요한 이유:

- 배치 처리: 여러 시퀀스를 동시에 처리할 때 길이를 맞추기 위해 사용됩니다.
- 고정 길이 입력: 많은 모델들이 고정된 길이의 입력을 요구합니다.
- 계산 효율성: 동일한 크기의 텐서로 처리하면 병렬 계산이 가능해집니다.

"나는 학교에 간다 [PAD] [PAD]" (3토큰) [1, 1, 1, 0, 0]
"그는 오늘 아침 일찍 출근했다" (5토큰) [1, 1, 1, 1, 1]

# 왜 시퀀스의 길이를 맞춰야 하는가?

1. 배치 처리의 효율성:

   - 딥러닝 모델은 보통 배치 단위로 데이터를 처리합니다.
   - 배치 내의 모든 시퀀스가 같은 길이여야 효율적인 병렬 처리가 가능합니다.

2. 텐서 연산의 요구사항:

   - 대부분의 딥러닝 프레임워크는 고정된 크기의 텐서를 사용합니다.
   - 가변 길이 시퀀스를 하나의 텐서로 처리하기 어렵습니다.

3. 메모리 관리:

   - 고정 길이 시퀀스를 사용하면 메모리 할당과 관리가 더 쉽고 효율적입니다.

4. 모델 아키텍처:

   - 많은 신경망 모델들이 고정된 입력 크기를 요구합니다.
   - 특히 fully connected 레이어는 고정된 입력 차원을 필요로 합니다.

5. 계산 최적화:

   - GPU나 TPU 같은 하드웨어는 균일한 크기의 데이터에 대해 최적화되어 있습니다.

6. 일관된 처리:
   - 모든 시퀀스를 동일한 방식으로 처리할 수 있어 코드 구현이 단순해집니다.
